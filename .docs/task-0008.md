# Task 0008: Create background job processing system

## Phase
Phase 2: Core Processing (Weeks 3-4)

## Description
Implement a robust background job processing system for handling AI inference and other asynchronous tasks.

## Acceptance Criteria
- [ ] Create job queue system with worker pool
- [ ] Implement job types (activity_processing, profile_generation)
- [ ] Add job status tracking and updates
- [ ] Implement retry logic with exponential backoff
- [ ] Create dead letter queue for failed jobs
- [ ] Add job priority and scheduling
- [ ] Implement graceful worker shutdown
- [ ] Add job monitoring and metrics

### Implementation status (updates)

- [x] Job queue system with worker pool implemented in `internal/jobs` (`WorkerPool`, `Repository`, `Job` model).
- [x] Job persistence & schema added: migration `db/migrations/0003_jobs.sql` (creates `jobs` and `dead_letter_jobs`).
- [x] Job status tracking, attempts, priority, scheduling and `next_try_at` implemented in DB and repository methods.
- [x] Retry logic with exponential backoff implemented (`BackoffDuration`) and retry scheduling in worker loop.
- [x] Dead letter queue implemented (`dead_letter_jobs`) and `MoveToDeadLetter` repository helper.
- [x] Graceful shutdown supported: `WorkerPool.Stop()` and context cancellation handling in workers.
- [~] Monitoring/metrics: structured logging added; suggest adding Prometheus counters or a metrics interface as a follow-up.

### Files added

- `db/migrations/0003_jobs.sql` — migration for jobs and dead-letter tables.
- `internal/jobs/jobs.go` — Job model, backoff helper, handler type.
- `internal/jobs/repository.go` — DB persistence: Enqueue, FetchNext, UpdateJob, MoveToDeadLetter.
- `internal/jobs/worker.go` — WorkerPool implementation, run loop, enqueue helper.
- `internal/jobs/jobs_test.go` — Test for enqueue and processing (uses shared in-memory SQLite).

### Test & build status

- Ran `go test ./internal/jobs` and `go test ./...` locally — all tested packages passed.

### Next steps

- Wire concrete handlers for `activity_processing` and `profile_generation` into application initialization (e.g., `cmd/server`) and call AI inference code from `internal/ai`.
- Add Prometheus metrics or a metrics interface for job throughput, retries, failures, and queue depth.
- Add more tests covering:
	- retry behavior and dead-letter moves
	- priority and scheduling ordering
	- graceful shutdown with in-flight jobs

If desired, I can implement any of the next steps; tell me which one to start with.

## Dependencies
- Task 0005 (Data models)
- Task 0007 (AI inference engine)

## Estimated Effort
4-5 days

## Technical Notes
- Use Go channels and goroutines for job processing
- Implement configurable worker count
- Add proper context handling for job cancellation
- Use database for job persistence and status tracking
- Implement exponential backoff for retries
- Add comprehensive logging for job lifecycle
- Consider using existing job queue library (e.g., asynq)

## Definition of Done
- Job queue can process tasks asynchronously
- Worker pool management is functional
- Retry logic handles transient failures
- Job status tracking works correctly
- Dead letter queue captures permanent failures
- System can gracefully shut down workers
